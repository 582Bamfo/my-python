#Machine learning (ML) is a branch of artificial intelligence (AI) that involves the development of algorithms and statistical models that enable computers to learn from and make predictions or decisions based on data. Unlike traditional programming, where specific instructions are written to perform a task, ML algorithms use patterns and inference to improve their performance on a task over time.

#Key Concepts in Machine Learning
#Supervised Learning: In supervised learning, the algorithm is trained on labeled data, where the input-output pairs are provided. The goal is to learn a mapping from inputs to outputs. Common algorithms include:

# Linear Regression
# Logistic Regression
# Support Vector Machines (SVM)
# Decision Trees
# Random Forests
# Neural Networks
# Unsupervised Learning: Here, the algorithm is trained on unlabeled data and must find patterns and relationships within the data. Common techniques include:

# Clustering (e.g., K-Means, Hierarchical Clustering)
# Dimensionality Reduction (e.g., Principal Component Analysis, t-SNE)
# Reinforcement Learning: In this paradigm, an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Key concepts include:

# Markov Decision Processes (MDPs)
# Q-Learning
# Deep Q-Networks (DQNs)
# Policy Gradient Methods
# Semi-Supervised Learning: Combines a small amount of labeled data with a large amount of unlabeled data during training. This is particularly useful when labeling data is expensive or time-consuming.

# Self-Supervised Learning: A type of unsupervised learning where the system generates labels from the input data itself, allowing the model to learn representations of the data without explicit labels.

# Transfer Learning: Utilizes knowledge gained while solving one problem and applies it to a different but related problem. This is common in deep learning, where models trained on large datasets like ImageNet can be fine-tuned for specific tasks.

# Key Steps in a Machine Learning Project
# Problem Definition: Understand the problem and define the goal.
# Data Collection: Gather data relevant to the problem.
# Data Preprocessing: Clean, normalize, and transform data into a suitable format.
# Feature Engineering: Select or create meaningful features that will help the model learn effectively.
# Model Selection: Choose the appropriate machine learning algorithm for the task.
# Training: Train the model on the training data.
# Evaluation: Assess the model's performance using validation or test data.
# Hyperparameter Tuning: Optimize the model parameters to improve performance.
# Deployment: Implement the model in a production environment.
# Monitoring and Maintenance: Continuously monitor the model's performance and update as necessary.
# Popular Machine Learning Libraries and Tools
# Scikit-learn: A Python library for simple and efficient tools for data mining and data analysis.
# TensorFlow: An open-source library for machine learning and deep learning applications, developed by Google.
# PyTorch: An open-source machine learning library developed by Facebook's AI Research lab.
# Keras: An open-source software library that provides a Python interface for artificial neural networks, built on top of TensorFlow.
# XGBoost: An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable.
# Applications of Machine Learning
# Natural Language Processing (NLP): Language translation, sentiment analysis, speech recognition.
# Computer Vision: Image recognition, object detection, facial recognition.
# Healthcare: Disease prediction, personalized treatment, medical image analysis.
# Finance: Fraud detection, algorithmic trading, credit scoring.
# Marketing: Customer segmentation, recommendation systems, churn prediction.
# Autonomous Systems: Self-driving cars, robotics.




#steps
#1 - Import the data eg pandas
#2 - Clean the data
#3 - Split data. Training Set/Test Set
#4 - Create a Model eg k nearest neighbour
#5 - Check the output
#6 - Improve

#facebook machine learning
#https://research.facebook.com/blog/2018/05/the-facebook-field-guide-to-machine-learning-video-series/


Tools needed in ML
# To effectively work on machine learning projects, you need a variety of tools and frameworks that cater to different stages of the machine learning pipeline. Here are some essential tools and libraries used in machine learning:

# To effectively work on machine learning projects, you need a variety of tools and frameworks that cater to different stages of the machine learning pipeline. Here are some essential tools and libraries used in machine learning:

# Programming Languages
# Python: The most popular language for machine learning due to its simplicity and extensive libraries.
# R: Widely used for statistical analysis and data visualization.
              
# Libraries and Frameworks
# Data Manipulation and Analysis
# Pandas: Provides data structures and data analysis tools for Python.
# NumPy: Supports large, multi-dimensional arrays and matrices, along with a collection of mathematical functions.
# Dask: Handles large datasets by parallelizing operations across multiple cores or clusters.

#   Machine Learning
# Scikit-learn: Offers simple and efficient tools for data mining and data analysis.
# TensorFlow: An end-to-end open-source platform for machine learning, developed by Google.
# PyTorch: An open-source machine learning library developed by Facebook's AI Research lab.
# Keras: Provides a high-level neural networks API, running on top of TensorFlow.
# XGBoost: An optimized gradient boosting library designed for speed and performance.
# LightGBM: A gradient boosting framework that uses tree-based learning algorithms.
# CatBoost: A gradient boosting library that handles categorical features automatically.

#   Data Visualization
# Matplotlib: A plotting library for creating static, animated, and interactive visualizations in Python.
# Seaborn: Built on top of Matplotlib, it provides a high-level interface for drawing attractive statistical graphics.
# Plotly: An interactive graphing library that makes it easy to create interactive plots.
# Tableau: A powerful and fast-growing data visualization tool used in the business intelligence industry.

#   Deep Learning
# TensorFlow: Mentioned above; widely used for deep learning applications.
# PyTorch: Mentioned above; known for its dynamic computation graph and ease of use.
# Keras: Mentioned above; simplifies building and training deep learning models.
# Caffe: A deep learning framework made with expression, speed, and modularity in mind.
# MXNet: A deep learning framework designed for both efficiency and flexibility.

#                                          Data Collection and Preparation
# Scrapy: An open-source and collaborative web crawling framework for Python.
# Beautiful Soup: A library for parsing HTML and XML documents and extracting data.
# OpenCV: A library of programming functions mainly aimed at real-time computer vision.

#                                             Model Deployment
# Flask: A lightweight WSGI web application framework in Python.
# Django: A high-level Python web framework that encourages rapid development and clean, pragmatic design.
# Docker: A platform to develop, ship, and run applications inside containers.
# Kubernetes: An open-source system for automating the deployment, scaling, and management of containerized applications.

#                                             Experiment Tracking and Management
# MLflow: An open-source platform to manage the ML lifecycle, including experimentation, reproducibility, and deployment.
# Weights & Biases: Provides tools to track experiments, visualize results, and share insights.
# TensorBoard: A suite of visualization tools for TensorFlow.

#                                             Automated Machine Learning (AutoML)
# AutoKeras: An open-source AutoML library built on Keras.
# TPOT: A Python tool that automatically creates and optimizes machine learning pipelines.
# H2O.ai: An open-source platform that makes it easy to build machine learning models on big data.

#                                             Integrated Development Environments (IDEs) and Notebooks
# Jupyter Notebook: An open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. it comes with preinstall library
# Google Colab: A Jupyter notebook environment that runs in the cloud and offers free access to GPUs.
# PyCharm: A Python IDE with a complete set of tools for productive development.
# VSCode: A free, open-source code editor with support for debugging, embedded Git control, syntax highlighting, intelligent code completion, snippets, and more.
# These tools collectively support various stages of a machine learning project, from data collection and preprocessing to model building, training, evaluation, and deployment. Choosing the right tool depends on the specific requirements of your project and your familiarity with the tools.


https://www.kaggle.com/
# kaggle is a community of ML expert where you can extract data, see other expert modules  and learn


Kaggle Datasets
In the next video we will use a website called Kaggle to download some fun datasets. I have attached the CSV file to that lecture for you.  However....

You can also try downloading your own data and see if you can follow along (but the organization of data may be different so you have to use some problem solving skills. For example, you can search Kaggle and find a more up to date: FIFA 2023 Data

pandas cheatsheet
https://elitedatascience.com/python-cheat-sheet

#visualisation
https://seaborn.pydata.org/
https://docs.bokeh.org/en/latest/index.html

#creating module.
https://scikit-learn.org/stable/modules/neighbors.html


#Follow this for your learning
https://github.com/aneagoie/ML-Notes/blob/master/iris.ipynb